{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import queue\n",
    "import pprint\n",
    "import random\n",
    "import argparse\n",
    "import importlib\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from config import system_configs\n",
    "from nnet.py_factory import NetworkFactory\n",
    "from torch.multiprocessing import Process, Queue, Pool\n",
    "from db.datasets import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled   = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefetch_data(db, queue, sample_data, data_aug):\n",
    "    ind = 0\n",
    "    print(\"start prefetching data...\")\n",
    "    np.random.seed(os.getpid())\n",
    "    while True:\n",
    "        try:\n",
    "            data, ind = sample_data(db, ind, data_aug=data_aug)\n",
    "            queue.put(data)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "\n",
    "def pin_memory(data_queue, pinned_data_queue, sema):\n",
    "    while True:\n",
    "        data = data_queue.get()\n",
    "\n",
    "        data[\"xs\"] = [x.pin_memory() for x in data[\"xs\"]]\n",
    "        data[\"ys\"] = [y.pin_memory() for y in data[\"ys\"]]\n",
    "\n",
    "        pinned_data_queue.put(data)\n",
    "\n",
    "        if sema.acquire(blocking=False):\n",
    "            return\n",
    "\n",
    "def init_parallel_jobs(dbs, queue, fn, data_aug):\n",
    "    tasks = [Process(target=prefetch_data, args=(db, queue, fn, data_aug)) for db in dbs]\n",
    "    for task in tasks:\n",
    "        task.daemon = True\n",
    "        task.start()\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_dbs, validation_db, start_iter=0):\n",
    "    learning_rate    = system_configs.learning_rate\n",
    "    max_iteration    = system_configs.max_iter\n",
    "    pretrained_model = system_configs.pretrain\n",
    "    snapshot         = system_configs.snapshot\n",
    "    val_iter         = system_configs.val_iter\n",
    "    display          = system_configs.display\n",
    "    decay_rate       = system_configs.decay_rate\n",
    "    stepsize         = system_configs.stepsize\n",
    "\n",
    "    # getting the size of each database\n",
    "    training_size   = len(training_dbs[0].db_inds)\n",
    "    validation_size = len(validation_db.db_inds)\n",
    "\n",
    "    # queues storing data for training\n",
    "    training_queue   = Queue(system_configs.prefetch_size)\n",
    "    validation_queue = Queue(5)\n",
    "\n",
    "    # queues storing pinned data for training\n",
    "    pinned_training_queue   = queue.Queue(system_configs.prefetch_size)\n",
    "    pinned_validation_queue = queue.Queue(5)\n",
    "\n",
    "    # load data sampling function\n",
    "    data_file   = \"sample.{}\".format(training_dbs[0].data)\n",
    "    sample_data = importlib.import_module(data_file).sample_data\n",
    "\n",
    "    # allocating resources for parallel reading\n",
    "    training_tasks   = init_parallel_jobs(training_dbs, training_queue, sample_data, True)\n",
    "    if val_iter:\n",
    "        validation_tasks = init_parallel_jobs([validation_db], validation_queue, sample_data, False)\n",
    "\n",
    "    training_pin_semaphore   = threading.Semaphore()\n",
    "    validation_pin_semaphore = threading.Semaphore()\n",
    "    training_pin_semaphore.acquire()\n",
    "    validation_pin_semaphore.acquire()\n",
    "\n",
    "    training_pin_args   = (training_queue, pinned_training_queue, training_pin_semaphore)\n",
    "    training_pin_thread = threading.Thread(target=pin_memory, args=training_pin_args)\n",
    "    training_pin_thread.daemon = True\n",
    "    training_pin_thread.start()\n",
    "\n",
    "    validation_pin_args   = (validation_queue, pinned_validation_queue, validation_pin_semaphore)\n",
    "    validation_pin_thread = threading.Thread(target=pin_memory, args=validation_pin_args)\n",
    "    validation_pin_thread.daemon = True\n",
    "    validation_pin_thread.start()\n",
    "\n",
    "    print(\"building model...\")\n",
    "    nnet = NetworkFactory(training_dbs[0])\n",
    "\n",
    "    if pretrained_model is not None:\n",
    "        if not os.path.exists(pretrained_model):\n",
    "            raise ValueError(\"pretrained model does not exist\")\n",
    "        print(\"loading from pretrained model\")\n",
    "        nnet.load_pretrained_params(pretrained_model)\n",
    "\n",
    "    if start_iter:\n",
    "        learning_rate /= (decay_rate ** (start_iter // stepsize))\n",
    "\n",
    "        nnet.load_params(start_iter)\n",
    "        nnet.set_lr(learning_rate)\n",
    "        print(\"training starts from iteration {} with learning_rate {}\".format(start_iter + 1, learning_rate))\n",
    "    else:\n",
    "        nnet.set_lr(learning_rate)\n",
    "\n",
    "    print(\"training start...\")\n",
    "    nnet.cuda()\n",
    "    nnet.train_mode()\n",
    "    for iteration in tqdm(range(start_iter + 1, max_iteration + 1)):\n",
    "        training = pinned_training_queue.get(block=True)\n",
    "        training_loss, focal_loss, pull_loss, push_loss, regr_loss = nnet.train(**training)\n",
    "        #training_loss, focal_loss, pull_loss, push_loss, regr_loss, cls_loss = nnet.train(**training)\n",
    "\n",
    "        if display and iteration % display == 0:\n",
    "            print(\"training loss at iteration {}: {}\".format(iteration, training_loss.item()))\n",
    "            print(\"focal loss at iteration {}:    {}\".format(iteration, focal_loss.item()))\n",
    "            print(\"pull loss at iteration {}:     {}\".format(iteration, pull_loss.item())) \n",
    "            print(\"push loss at iteration {}:     {}\".format(iteration, push_loss.item()))\n",
    "            print(\"regr loss at iteration {}:     {}\".format(iteration, regr_loss.item()))\n",
    "            #print(\"cls loss at iteration {}:      {}\\n\".format(iteration, cls_loss.item()))\n",
    "\n",
    "        del training_loss, focal_loss, pull_loss, push_loss, regr_loss#, cls_loss\n",
    "\n",
    "        if val_iter and validation_db.db_inds.size and iteration % val_iter == 0:\n",
    "            nnet.eval_mode()\n",
    "            validation = pinned_validation_queue.get(block=True)\n",
    "            validation_loss = nnet.validate(**validation)\n",
    "            print(\"validation loss at iteration {}: {}\".format(iteration, validation_loss.item()))\n",
    "            nnet.train_mode()\n",
    "\n",
    "        if iteration % snapshot == 0:\n",
    "            nnet.save_params(iteration)\n",
    "\n",
    "        if iteration % stepsize == 0:\n",
    "            learning_rate /= decay_rate\n",
    "            nnet.set_lr(learning_rate)\n",
    "\n",
    "    # sending signal to kill the thread\n",
    "    training_pin_semaphore.release()\n",
    "    validation_pin_semaphore.release()\n",
    "\n",
    "    # terminating data fetching processes\n",
    "    for training_task in training_tasks:\n",
    "        training_task.terminate()\n",
    "    for validation_task in validation_tasks:\n",
    "        validation_task.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all datasets...\n",
      "using 1 threads\n",
      "loading from cache file: cache/coco_trainval2014.pkl\n",
      "loading annotations into memory...\n",
      "Done (t=7.85s)\n",
      "creating index...\n",
      "index created!\n",
      "loading from cache file: cache/coco_minival2014.pkl\n",
      "loading annotations into memory...\n",
      "Done (t=0.87s)\n",
      "creating index...\n",
      "index created!\n",
      "system config...\n",
      "{'batch_size': 4,\n",
      " 'cache_dir': 'cache',\n",
      " 'chunk_sizes': [6, 6, 6, 6, 6, 6, 6, 6],\n",
      " 'config_dir': 'config',\n",
      " 'data_dir': './data',\n",
      " 'data_rng': RandomState(MT19937) at 0x7F7B8BB62A98,\n",
      " 'dataset': 'MSCOCO',\n",
      " 'decay_rate': 10,\n",
      " 'display': 5,\n",
      " 'learning_rate': 0.00025,\n",
      " 'max_iter': 480000,\n",
      " 'nnet_rng': RandomState(MT19937) at 0x7F7B8BB62CA8,\n",
      " 'opt_algo': 'adam',\n",
      " 'prefetch_size': 6,\n",
      " 'pretrain': None,\n",
      " 'result_dir': 'results',\n",
      " 'sampling_function': 'kp_detection',\n",
      " 'snapshot': 5000,\n",
      " 'snapshot_name': 'CenterNet-52',\n",
      " 'stepsize': 450000,\n",
      " 'test_split': 'testdev',\n",
      " 'train_split': 'trainval',\n",
      " 'val_iter': 500,\n",
      " 'val_split': 'minival',\n",
      " 'weight_decay': False,\n",
      " 'weight_decay_rate': 1e-05,\n",
      " 'weight_decay_type': 'l2'}\n",
      "db config...\n",
      "{'ae_threshold': 0.5,\n",
      " 'border': 128,\n",
      " 'categories': 80,\n",
      " 'data_aug': True,\n",
      " 'gaussian_bump': True,\n",
      " 'gaussian_iou': 0.7,\n",
      " 'gaussian_radius': -1,\n",
      " 'input_size': [511, 511],\n",
      " 'kp_categories': 1,\n",
      " 'lighting': True,\n",
      " 'max_per_image': 100,\n",
      " 'merge_bbox': False,\n",
      " 'nms_algorithm': 'exp_soft_nms',\n",
      " 'nms_kernel': 3,\n",
      " 'nms_threshold': 0.5,\n",
      " 'output_sizes': [[128, 128]],\n",
      " 'rand_color': True,\n",
      " 'rand_crop': True,\n",
      " 'rand_pushes': False,\n",
      " 'rand_samples': False,\n",
      " 'rand_scale_max': 1.4,\n",
      " 'rand_scale_min': 0.6,\n",
      " 'rand_scale_step': 0.1,\n",
      " 'rand_scales': array([0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3]),\n",
      " 'special_crop': False,\n",
      " 'test_scales': [1],\n",
      " 'top_k': 70,\n",
      " 'weight_exp': 8}\n",
      "len of db: 118287\n"
     ]
    }
   ],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train CenterNet\")\n",
    "#     parser.add_argument(\"cfg_file\", help=\"config file\", type=str)\n",
    "#     parser.add_argument(\"--iter\", dest=\"start_iter\",\n",
    "#                         help=\"train at iteration i\",\n",
    "#                         default=0, type=int)\n",
    "#     parser.add_argument(\"--threads\", dest=\"threads\", default=4, type=int)\n",
    "\n",
    "#     #args = parser.parse_args()\n",
    "#     args, unparsed = parser.parse_known_args()\n",
    "#     return args\n",
    "# args = parse_args()\n",
    "\n",
    "args = {'cfg_file': 'CenterNet-52',\n",
    "        'start_iter': 0,\n",
    "        'threads': 1}\n",
    "\n",
    "cfg_file = os.path.join(system_configs.config_dir, args['cfg_file'] + \".json\")\n",
    "with open(cfg_file, \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "configs[\"system\"][\"snapshot_name\"] = args['cfg_file']\n",
    "system_configs.update_config(configs[\"system\"])\n",
    "\n",
    "train_split = system_configs.train_split\n",
    "val_split   = system_configs.val_split\n",
    "\n",
    "print(\"loading all datasets...\")\n",
    "dataset = system_configs.dataset\n",
    "# threads = max(torch.cuda.device_count() * 2, 4)\n",
    "threads = args['threads']\n",
    "print(\"using {} threads\".format(threads))\n",
    "training_dbs  = [datasets[dataset](configs[\"db\"], train_split) for _ in range(threads)]\n",
    "validation_db = datasets[dataset](configs[\"db\"], val_split)\n",
    "\n",
    "print(\"system config...\")\n",
    "pprint.pprint(system_configs.full)\n",
    "\n",
    "print(\"db config...\")\n",
    "pprint.pprint(training_dbs[0].configs)\n",
    "\n",
    "print(\"len of db: {}\".format(len(training_dbs[0].db_inds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prefetching data...\n",
      "shuffling indices...\n",
      "start prefetching data...\n",
      "shuffling indices...\n",
      "building model...\n",
      "module_file: models.CenterNet-52\n",
      "total parameters: 104844152\n",
      "setting learning rate to: 0.00025\n",
      "training start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1ddc66355e410daa450d9a91ba9573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=480000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at iteration 5: 420.2887878417969\n",
      "focal loss at iteration 5:    419.49114990234375\n",
      "pull loss at iteration 5:     0.016489481553435326\n",
      "push loss at iteration 5:     0.33171120285987854\n",
      "regr loss at iteration 5:     0.44944384694099426\n",
      "training loss at iteration 10: 162.9038543701172\n",
      "focal loss at iteration 10:    162.19921875\n",
      "pull loss at iteration 10:     0.02546476200222969\n",
      "push loss at iteration 10:     0.332808256149292\n",
      "regr loss at iteration 10:     0.346366822719574\n",
      "training loss at iteration 15: 149.5183563232422\n",
      "focal loss at iteration 15:    148.78408813476562\n",
      "pull loss at iteration 15:     0.08973975479602814\n",
      "push loss at iteration 15:     0.3529437482357025\n",
      "regr loss at iteration 15:     0.29157406091690063\n",
      "training loss at iteration 20: 21.524978637695312\n",
      "focal loss at iteration 20:    20.76079559326172\n",
      "pull loss at iteration 20:     0.1051899716258049\n",
      "push loss at iteration 20:     0.3286711871623993\n",
      "regr loss at iteration 20:     0.3303218483924866\n",
      "training loss at iteration 25: 22.107717514038086\n",
      "focal loss at iteration 25:    21.416637420654297\n",
      "pull loss at iteration 25:     0.0710073709487915\n",
      "push loss at iteration 25:     0.34857049584388733\n",
      "regr loss at iteration 25:     0.2715027928352356\n",
      "training loss at iteration 30: 14.436393737792969\n",
      "focal loss at iteration 30:    13.863977432250977\n",
      "pull loss at iteration 30:     0.020195579156279564\n",
      "push loss at iteration 30:     0.2883549928665161\n",
      "regr loss at iteration 30:     0.26386576890945435\n",
      "training loss at iteration 35: 17.918119430541992\n",
      "focal loss at iteration 35:    17.28371810913086\n",
      "pull loss at iteration 35:     0.07201088964939117\n",
      "push loss at iteration 35:     0.34204840660095215\n",
      "regr loss at iteration 35:     0.22034256160259247\n",
      "training loss at iteration 40: 17.224773406982422\n",
      "focal loss at iteration 40:    16.696069717407227\n",
      "pull loss at iteration 40:     0.06001562997698784\n",
      "push loss at iteration 40:     0.28467410802841187\n",
      "regr loss at iteration 40:     0.1840151846408844\n",
      "training loss at iteration 45: 14.434163093566895\n",
      "focal loss at iteration 45:    13.785576820373535\n",
      "pull loss at iteration 45:     0.12668554484844208\n",
      "push loss at iteration 45:     0.2815937399864197\n",
      "regr loss at iteration 45:     0.2403082549571991\n",
      "training loss at iteration 50: 14.754996299743652\n",
      "focal loss at iteration 50:    14.131053924560547\n",
      "pull loss at iteration 50:     0.04608038067817688\n",
      "push loss at iteration 50:     0.3244937062263489\n",
      "regr loss at iteration 50:     0.2533682584762573\n",
      "training loss at iteration 55: 13.856679916381836\n",
      "focal loss at iteration 55:    13.256568908691406\n",
      "pull loss at iteration 55:     0.042224712669849396\n",
      "push loss at iteration 55:     0.29332977533340454\n",
      "regr loss at iteration 55:     0.2645556330680847\n",
      "training loss at iteration 60: 15.107327461242676\n",
      "focal loss at iteration 60:    14.470836639404297\n",
      "pull loss at iteration 60:     0.04574015736579895\n",
      "push loss at iteration 60:     0.34077367186546326\n",
      "regr loss at iteration 60:     0.24997678399085999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-d0866c996085>\", line 8, in prefetch_data\n",
      "    queue.put(data)\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-4-d0866c996085>\", line 8, in prefetch_data\n",
      "    queue.put(data)\n",
      "  File \"/home/teng/miniconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 82, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9a39e9220f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d055d886509e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_dbs, validation_db, start_iter)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinned_training_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mtraining_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpull_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpush_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m#training_loss, focal_loss, pull_loss, push_loss, regr_loss, cls_loss = nnet.train(**training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CenterNet-Keypoint/nnet/py_factory.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, xs, ys, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mloss_kp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mloss\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mloss_kp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mfocal_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mloss_kp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CenterNet-Keypoint/nnet/py_factory.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, ys, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss_kp\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_kp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CenterNet-Keypoint/models/py_utils/kp.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, outs, targets)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mct_heats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mct_heats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mfocal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_heats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_tl_heat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mfocal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbr_heats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_br_heat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mfocal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct_heats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_ct_heat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CenterNet-Keypoint/models/py_utils/kp_utils.py\u001b[0m in \u001b[0;36m_neg_loss\u001b[0;34m(preds, gt)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mneg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mneg_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneg_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(training_dbs, validation_db, args['start_iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
